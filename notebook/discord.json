{
	"name": "discord",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SampleSpark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "35e87574-214b-4c20-879e-1b8ff923c344"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7ee734d7-98fe-4bd4-ad7f-a30816ebebeb/resourceGroups/rg-surfalytics-analytics-westus2/providers/Microsoft.Synapse/workspaces/synapse-001/bigDataPools/SampleSpark",
				"name": "SampleSpark",
				"type": "Spark",
				"endpoint": "https://synapse-001.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SampleSpark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"pip install discord pandas pyarrow pyspark adlfs aiohttp asyncio nest_asyncio"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import os\n",
					"import discord\n",
					"import pandas as pd\n",
					"import aiohttp\n",
					"import asyncio\n",
					"import pyarrow as pa\n",
					"import pyarrow.parquet as pq\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.types import *\n",
					"import nest_asyncio\n",
					"import adlfs\n",
					"\n",
					"# Apply nest_asyncio patch to allow running nested event loops\n",
					"nest_asyncio.apply()\n",
					"\n",
					"# Initialize Spark session\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"DiscordDataToADLS\") \\\n",
					"    .getOrCreate()\n",
					"\n",
					"# Environment variables\n",
					"token = ''\n",
					"guild_id = ''\n",
					"account_name = ''\n",
					"account_key = ''\n",
					"container_name = 'datalstorage'  # replace with your container name\n",
					"relative_path = 'synapse/workspaces/synapse-001/warehouse/discord/'  # replace with your relative path\n",
					"\n",
					"headers = {\n",
					"    'Authorization': f'Bot {token}',\n",
					"    'Content-Type': 'application/json'\n",
					"}\n",
					"\n",
					"# Construct ADLS path\n",
					"def get_adls_path(file_name):\n",
					"    return f'abfss://{container_name}@{account_name}.dfs.core.windows.net/{relative_path}{file_name}'\n",
					"\n",
					"class DiscordClient(discord.Client):\n",
					"    def __init__(self, *args, **kwargs):\n",
					"        super().__init__(*args, **kwargs)\n",
					"        self.df_invite = None\n",
					"\n",
					"    async def on_ready(self):\n",
					"        print(f'We have logged in as {self.user}')\n",
					"        await self.fetch_user_invite_codes()\n",
					"        await self.fetch_user_activity()\n",
					"        await self.close()\n",
					"\n",
					"    async def fetch_user_invite_codes(self):\n",
					"        url = f'https://discord.com/api/v9/guilds/{guild_id}/members-search'\n",
					"        all_invite_data = []\n",
					"        async with aiohttp.ClientSession(headers=headers) as session:\n",
					"            async with session.post(url, json={'limit': 1000}) as response:\n",
					"                if response.status == 200:\n",
					"                    data = await response.json()\n",
					"                    for member_info in data.get('members', []):\n",
					"                        user_info = member_info['member']['user']\n",
					"                        user_id = user_info['id']\n",
					"                        invite_code = member_info.get('source_invite_code', 'N/A')\n",
					"                        all_invite_data.append({\n",
					"                            'user_id': user_id,\n",
					"                            'source_invite_code': invite_code\n",
					"                        })\n",
					"                else:\n",
					"                    print(f'Error fetching invite data: {response.status}')\n",
					"\n",
					"        self.df_invite = pd.DataFrame(all_invite_data)\n",
					"        table_invite = pa.Table.from_pandas(self.df_invite)\n",
					"        invite_parquet_path = get_adls_path('user_invite_codes.parquet')\n",
					"        fs = adlfs.AzureBlobFileSystem(account_name=account_name, account_key=account_key)\n",
					"        with fs.open(invite_parquet_path, 'wb') as f:\n",
					"            pq.write_table(table_invite, f)\n",
					"        print(f\"User invite codes saved to {invite_parquet_path}\")\n",
					"\n",
					"    async def fetch_user_activity(self):\n",
					"        all_user_data = []\n",
					"\n",
					"        for guild in self.guilds:\n",
					"            print(f'Fetching data for guild: {guild.name}')\n",
					"            for channel in guild.text_channels:\n",
					"                print(f'Fetching data for channel: {channel.name}')\n",
					"                try:\n",
					"                    async for message in channel.history(limit=None):\n",
					"                        if message.author.bot:\n",
					"                            continue\n",
					"                        user_data = {\n",
					"                            'guild_name': guild.name,\n",
					"                            'channel_name': channel.name,\n",
					"                            'user_id': message.author.id,\n",
					"                            'name': message.author.name,\n",
					"                            'discriminator': message.author.discriminator,\n",
					"                            'message_count': 1\n",
					"                        }\n",
					"                        all_user_data.append(user_data)\n",
					"                except discord.Forbidden:\n",
					"                    print(f\"Cannot access channel: {channel.name}\")\n",
					"\n",
					"        df_activity = pd.DataFrame(all_user_data)\n",
					"        table_activity = pa.Table.from_pandas(df_activity)\n",
					"        activity_parquet_path = get_adls_path('user_message_counts.parquet')\n",
					"        fs = adlfs.AzureBlobFileSystem(account_name=account_name, account_key=account_key)\n",
					"        with fs.open(activity_parquet_path, 'wb') as f:\n",
					"            pq.write_table(table_activity, f)\n",
					"        print(f\"User message counts saved to {activity_parquet_path}\")\n",
					"\n",
					"        # Merge user activity data with invite data\n",
					"        try:\n",
					"            if self.df_invite is None:\n",
					"                raise ValueError(\"Invite data not fetched.\")\n",
					"            \n",
					"            # Ensure columns are present and of correct type\n",
					"            if 'user_id' not in df_activity.columns or 'user_id' not in self.df_invite.columns:\n",
					"                raise ValueError(\"Column 'user_id' is missing from one of the DataFrames.\")\n",
					"            \n",
					"            df_activity['user_id'] = df_activity['user_id'].astype(str)\n",
					"            self.df_invite['user_id'] = self.df_invite['user_id'].astype(str)\n",
					"            \n",
					"            # Merge DataFrames\n",
					"            df_merged = pd.merge(df_activity, self.df_invite, on='user_id', how='left')\n",
					"            \n",
					"            # Write merged DataFrame to ADLS\n",
					"            table_merged = pa.Table.from_pandas(df_merged)\n",
					"            combined_parquet_path = get_adls_path('user_combined_data.parquet')\n",
					"            with fs.open(combined_parquet_path, 'wb') as f:\n",
					"                pq.write_table(table_merged, f)\n",
					"            \n",
					"            print(f\"User combined data saved to {combined_parquet_path}\")\n",
					"        except Exception as e:\n",
					"            print(f\"Error merging data: {e}\")\n",
					"\n",
					"# Run the Discord client\n",
					"async def main():\n",
					"    client = DiscordClient(intents=discord.Intents.default())\n",
					"    await client.start(token)\n",
					"\n",
					"if __name__ == \"__main__\":\n",
					"    asyncio.get_event_loop().run_until_complete(main())\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			}
		]
	}
}